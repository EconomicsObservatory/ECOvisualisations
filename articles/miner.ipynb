{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "69247a50-0990-43d3-bed1-eed00095e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, pandas as pd, json, re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f920a-c6d0-4c2c-9a6e-94756b79784b",
   "metadata": {},
   "source": [
    "## Parse list of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "8f44423f-889d-4cbb-86b8-f8ca445cc535",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.economicsobservatory.com/answers/\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "nrOfPages = int(\n",
    "    soup.find(\"section\", {\"class\": \"answers__listing\"})\n",
    "    .find(\"div\", {\"class\": \"pagination\"})\n",
    "    .find(\"span\")\n",
    "    .text.split(\" of \")[1]\n",
    "    .split(\" \")[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "92c9da0a-9527-4772-b139-85ca339771e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "# articles = json.loads(open(\"articles.json\", \"r\").read())\n",
    "articles = {i[\"name\"]: i for i in articles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "9766b121-ba0e-4916-b825-f92034ef203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  of  9\n",
      "2  of  9\n",
      "3  of  9\n",
      "4  of  9\n",
      "5  of  9\n",
      "6  of  9\n",
      "7  of  9\n",
      "8  of  9\n",
      "9  of  9\n"
     ]
    }
   ],
   "source": [
    "for page in range(1, nrOfPages + 1):\n",
    "    url = \"https://www.economicsobservatory.com/answers/page/\" + str(page)\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    for k in (\n",
    "        soup.find(\"section\", {\"class\": \"answers__listing\"})\n",
    "        .find(\"div\", {\"class\": \"answers__listing-left\"})\n",
    "        .find(\"ul\")\n",
    "        .findAll(\"li\")\n",
    "    ):\n",
    "        paragraphs = k.find(\"div\").findAll(\"div\")\n",
    "        para0 = paragraphs[0].text.split(\" • \")\n",
    "        category = para0[0].strip()\n",
    "        date = str(pd.to_datetime(para0[1]))[:10]\n",
    "        title = paragraphs[1].find(\"a\").text\n",
    "        link = paragraphs[1].find(\"a\")[\"href\"]\n",
    "        name = link.split(\"/\")[-1]\n",
    "        if name not in articles:\n",
    "            articles[name] = {\n",
    "                \"name\": name,\n",
    "                \"category\": category,\n",
    "                \"date\": date,\n",
    "                \"title\": title,\n",
    "                \"url\": link,\n",
    "            }\n",
    "    print(page, \" of \", nrOfPages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f91efe1-a691-48f3-a8d1-e4b9e4dbf749",
   "metadata": {},
   "source": [
    "Make local backup or article metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "e56841f8-b568-4d07-aba2-510049e8231e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118909"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"articles.json\", \"w\").write(json.dumps(list(articles.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8e3ea-b555-4709-a50c-18ca9b9caa74",
   "metadata": {},
   "source": [
    "## Parse individual articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "9da9ff24-f202-4e3e-bd28-c7c59ac53663",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = []\n",
    "# authors = json.loads(open(\"authors.json\", \"r\").read())\n",
    "authors = {i[\"name\"]: i for i in authors}\n",
    "figures = []\n",
    "# figures = json.loads(open(\"figures.json\", \"r\").read())\n",
    "figures = {i[\"name\"]: i for i in figures}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9298492-1ee2-4f5d-8cbd-714f48c7ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in articles:\n",
    "    # for a in [\n",
    "    #     \"how-has-covid-19-affected-part-time-jobs\",\n",
    "    #     \"what-is-the-likely-impact-of-advertising-restrictions-on-obesity\",\n",
    "    # ]:\n",
    "    article = articles[a]\n",
    "    if \"text\" not in article:\n",
    "        # if True:\n",
    "\n",
    "        # Load article page\n",
    "        url = article[\"url\"]\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        # Extract content\n",
    "        relatedCategories = [\n",
    "            i.find(\"a\").text\n",
    "            for i in soup.find(\"ul\", {\"class\": \"article__sidebar-categories\"}).findAll(\n",
    "                \"li\"\n",
    "            )\n",
    "        ]\n",
    "        # articleIntro = soup.find(\"div\", {\"class\", \"article__intro\"}).find(\"h3\").text\n",
    "        if soup.find(\"div\", {\"class\", \"article__intro\"}):\n",
    "            articleIntro = re.sub(\n",
    "                \"\\xa0\",\n",
    "                \" \",\n",
    "                re.sub(\"\\n\", \"\", soup.find(\"div\", {\"class\", \"article__intro\"}).text),\n",
    "            ).strip()\n",
    "        articleText = \"\\n\".join(\n",
    "            [i.text for i in soup.find(\"div\", {\"class\", \"article__body\"}).findAll(\"p\")]\n",
    "        )\n",
    "\n",
    "        # Extract authors\n",
    "        articleAuthors = []\n",
    "        if soup.find(\"ul\", {\"class\": \"article__authors-list\"}):\n",
    "            for authorData in soup.find(\n",
    "                \"ul\", {\"class\": \"article__authors-list\"}\n",
    "            ).findAll(\"li\"):\n",
    "                author = authorData.find(\"div\", {\"class\": \"title\"}).text\n",
    "                articleAuthors.append(author)\n",
    "                if author not in authors:\n",
    "                    affiliation = authorData.find(\"div\", {\"class\": \"aff\"}).text\n",
    "                    profile = authorData.find(\"a\", {\"class\": \"link\"})[\"href\"]\n",
    "                    image = (\n",
    "                        authorData.find(\"div\", {\"class\", \"image\"})[\"style\"]\n",
    "                        .split(\"url\")[1]\n",
    "                        .split(\"'\")[1]\n",
    "                    )\n",
    "                    authors[author] = {\n",
    "                        \"name\": author,\n",
    "                        \"affiliation\": affiliation,\n",
    "                        \"profile\": profile,\n",
    "                        \"image\": image,\n",
    "                        \"articles\": [],\n",
    "                        \"expert\": [],\n",
    "                        \"categories\": [],\n",
    "                        \"related\": [],\n",
    "                    }\n",
    "                if article[\"name\"] not in authors[author][\"articles\"]:\n",
    "                    authors[author][\"articles\"].append(article[\"name\"])\n",
    "                if article[\"category\"] not in authors[author][\"categories\"]:\n",
    "                    authors[author][\"categories\"].append(article[\"category\"])\n",
    "                for relatedCategory in relatedCategories:\n",
    "                    if relatedCategory not in authors[author][\"related\"]:\n",
    "                        authors[author][\"related\"].append(relatedCategory)\n",
    "\n",
    "        # Extract figures\n",
    "        fig0 = \"Figure 1: \"\n",
    "        abc = \"abcdefghijklmnopqrstuv\"\n",
    "        counter = 0\n",
    "        articleFigures = []\n",
    "        paras = [i for i in soup.find(\"div\", {\"class\", \"article__body\"}).findAll(True)]\n",
    "        # Check the next nk elements after the heading is found\n",
    "        nk = 3\n",
    "        for p in range(0, len(paras) - nk):\n",
    "            if paras[p].name == \"h4\":\n",
    "                fig = paras[p].text\n",
    "                figFound = False\n",
    "                # Check the next nk elements after the heading is found\n",
    "                for k in range(1, nk + 1):\n",
    "                    # Static images found\n",
    "                    if paras[p + k].find(\"img\"):\n",
    "                        figSource = paras[p + k].find(\"img\")[\"src\"]\n",
    "                        figEmbed = \"img\"\n",
    "                        figType = \"image\"\n",
    "                        figFound = True\n",
    "                        break\n",
    "                    # Interactive embeds found\n",
    "                    if paras[p + k].name in [\"section\", \"figure\"]:\n",
    "                        if not paras[p + k].find(\"table\"):\n",
    "                            if \"wp-block-table\" not in paras[p + k][\"class\"]:\n",
    "                                if \"blocks__html\" in paras[p + k][\"class\"]:\n",
    "                                    if paras[p + k].find(\"iframe\"):\n",
    "                                        figSource = paras[p + k].find(\"iframe\")[\"src\"]\n",
    "                                        figEmbed = \"iframe\"\n",
    "                                        figType = \"d3plus\"\n",
    "                                    else:\n",
    "                                        scriptText = str(\n",
    "                                            paras[p + k].find(\"body\").find(\"script\")\n",
    "                                        )\n",
    "                                        figSource = scriptText[\n",
    "                                            scriptText.find(\"http\") : scriptText.find(\n",
    "                                                \"json\"\n",
    "                                            )\n",
    "                                            + 4\n",
    "                                        ]\n",
    "                                        figEmbed = \"iframe\"\n",
    "                                        figType = \"vega-lite\"\n",
    "                                    figFound = True\n",
    "                                    break\n",
    "                                elif \"wp-block-embed-youtube\" in paras[p + k][\"class\"]:\n",
    "                                    figSource = paras[p + k].find(\"iframe\")[\"src\"]\n",
    "                                    figEmbed = \"youtube-plugin\"\n",
    "                                    figType = \"video\"\n",
    "                                    figFound = True\n",
    "                                    break\n",
    "                                elif (\n",
    "                                    \"blocks__chart-svg\"\n",
    "                                    in paras[p + k].find(True)[\"class\"]\n",
    "                                ):\n",
    "                                    scriptText = str(paras[p + k].script)\n",
    "                                    if \"Plotly\" in scriptText:\n",
    "                                        figSource = scriptText[\n",
    "                                            scriptText.find(\">\")\n",
    "                                            + 1 : scriptText.find(\"</script\")\n",
    "                                        ].strip()\n",
    "                                        figEmbed = \"plotly-plugin\"\n",
    "                                        figType = \"plotly\"\n",
    "                                    elif \"var spec\" in scriptText:\n",
    "                                        figSource = json.loads(\n",
    "                                            scriptText[\n",
    "                                                scriptText.find(\"var spec\")\n",
    "                                                + 10 : scriptText.find(\"var view\")\n",
    "                                            ].strip()[:-1]\n",
    "                                        )\n",
    "                                        figEmbed = \"vega-lite-plugin\"\n",
    "                                        figType = \"vega-lite\"\n",
    "                                    figFound = True\n",
    "                                    break\n",
    "                if figFound:\n",
    "                    if \":\" not in fig:\n",
    "                        fig = fig0.split(\":\")[0] + abc[counter] + \": \" + fig\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        fig0 = fig\n",
    "                    figId = \"fig\" + fig.split(\":\")[0].split(\" \")[1]\n",
    "                    figName = article[\"name\"] + \"_\" + figId\n",
    "                    articleFigures.append(figName)\n",
    "                    if figName not in figures:\n",
    "                        figTitle = fig.split(\":\")[1].strip()\n",
    "                        figures[figName] = {\n",
    "                            \"name\": figName,\n",
    "                            \"title\": figTitle,\n",
    "                            \"type\": figType,\n",
    "                            \"source\": figSource,\n",
    "                            \"embed\": figEmbed,\n",
    "                            \"articles\": [article[\"name\"]],\n",
    "                        }\n",
    "                    elif article[\"name\"] not in figures[figName][\"articles\"]:\n",
    "                        figures[figName][\"articles\"].append(article[\"name\"])\n",
    "\n",
    "        # Extract experts\n",
    "        articleExperts = []\n",
    "        if soup.find(\"ul\", {\"class\": \"article__sidebar-experts\"}):\n",
    "            for authorData in soup.find(\n",
    "                \"ul\", {\"class\": \"article__sidebar-experts\"}\n",
    "            ).findAll(\"li\"):\n",
    "                author = authorData.find(\"div\", {\"class\": \"title\"}).text\n",
    "                articleExperts.append(author)\n",
    "                if author not in authors:\n",
    "                    affiliation = authorData.find(\"div\", {\"class\": \"aff\"}).text\n",
    "                    profile = authorData.find(\"a\", {\"class\": \"link\"})[\"href\"]\n",
    "                    image = (\n",
    "                        authorData.find(\"div\", {\"class\", \"image\"})[\"style\"]\n",
    "                        .split(\"url\")[1]\n",
    "                        .split(\"'\")[1]\n",
    "                    )\n",
    "                    authors[author] = {\n",
    "                        \"name\": author,\n",
    "                        \"affiliation\": affiliation,\n",
    "                        \"profile\": profile,\n",
    "                        \"image\": image,\n",
    "                        \"articles\": [],\n",
    "                        \"expert\": [],\n",
    "                        \"categories\": [],\n",
    "                        \"related\": [],\n",
    "                    }\n",
    "                if article[\"name\"] not in authors[author][\"expert\"]:\n",
    "                    authors[author][\"expert\"].append(article[\"name\"])\n",
    "                if article[\"category\"] not in authors[author][\"categories\"]:\n",
    "                    authors[author][\"categories\"].append(article[\"category\"])\n",
    "                for relatedCategory in relatedCategories:\n",
    "                    if relatedCategory not in authors[author][\"related\"]:\n",
    "                        authors[author][\"related\"].append(relatedCategory)\n",
    "\n",
    "        # Augment article data\n",
    "        article[\"authors\"] = articleAuthors\n",
    "        article[\"figures\"] = articleFigures\n",
    "        article[\"experts\"] = articleExperts\n",
    "        article[\"related\"] = relatedCategories\n",
    "        article[\"intro\"] = articleIntro\n",
    "        article[\"text\"] = articleText\n",
    "\n",
    "        print(article[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4085c67-f82d-43e9-8685-3ea39c97afc7",
   "metadata": {},
   "source": [
    "Update data on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "7838a04c-cbd0-4e58-87b5-cd8458a55212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4198013"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for a in articles:\n",
    "    articles[a]['wordCount']=len(articles[a]['intro'].split(' '))+len(articles[a]['text'].split(' '))\n",
    "    articles[a]['figureCount']=len(articles[a]['figures'])\n",
    "    articles[a]['authorCount']=len(articles[a]['authors'])\n",
    "    articles[a]['expertCount']=len(articles[a]['experts'])\n",
    "open(\"articles.json\", \"w\").write(json.dumps(list(articles.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "1e425f62-090e-4091-ac55-d964c067669d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1465824"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"figures.json\", \"w\").write(json.dumps(list(figures.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88460ccd-6fb1-439b-aebe-29591dd7c263",
   "metadata": {},
   "source": [
    "Clean up author affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "18916435-94cc-4e6c-8836-f0a56696c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "unis = []\n",
    "# uni = json.loads(open(\"unis.json\", \"r\").read())\n",
    "unis = {i[\"name\"]: i for i in unis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "a29c4154-7948-45cf-b468-fec3a857b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v.append(process.extractOne(s, list(df.columns))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "dbed2c49-e35d-4426-aab1-4e65ef32b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "42e24060-c842-420e-a31b-8f0e8c5bdf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "eb134024-49d5-4dbf-b84c-f0a992851bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAffiliation(name, aff, unis):\n",
    "    from fuzzywuzzy import fuzz\n",
    "    from fuzzywuzzy import process\n",
    "\n",
    "    uni = []\n",
    "    dept = []\n",
    "    aff = (\n",
    "        aff.strip()\n",
    "        .replace(\"&\", \",\")\n",
    "        .replace(\" and \", \",\")\n",
    "        .replace(\"/\", \",\")\n",
    "        .replace(\"’\", \"'\")\n",
    "        .replace(\"“\", \"\")\n",
    "        .replace(\"”\", \"\")\n",
    "        .replace(', Fullerton',' Fullerton')\n",
    "        .replace('Cambridge Judge','Cambridge, Judge')\n",
    "    )\n",
    "    for a in aff.lower().split(\",\"):\n",
    "        a = a.strip().title()\n",
    "        if len(a) < 5:\n",
    "            a = a.upper()\n",
    "        if a == \"LSE\":\n",
    "            a = \"London School of Economics\"\n",
    "        if a == \"WEF\":\n",
    "            a = \"World Economic Forum\"\n",
    "        if a=='CGD':\n",
    "            a='Center For Global Development'\n",
    "        elif \"University\" in a:\n",
    "            if a=='Create Fellow In Cultural Economics (University Of Glasgow)':\n",
    "                a='University Of Glasgow'\n",
    "            uni.append(a)\n",
    "            if a not in unis:\n",
    "                unis[a] = {\"name\": a}\n",
    "        elif a == \"Oxford\":\n",
    "            uni.append(\"University of Oxford\")\n",
    "        elif a == \"Cambridge\":\n",
    "            uni.append(\"University of Cambridge\")\n",
    "        elif \"College\" in a:\n",
    "            if a == \"Boston College\":\n",
    "                uni.append(a)\n",
    "            else:\n",
    "                dept.append(a)\n",
    "        elif \"School\" in a:\n",
    "            if a == \"London School of Economics\":\n",
    "                uni.append(a)\n",
    "            else:\n",
    "                dept.append(a)\n",
    "        elif \"Centre\" in a:\n",
    "            dept.append(a)\n",
    "        elif \"Center\" in a:\n",
    "            dept.append(a)\n",
    "        elif \"Institut\" in a:\n",
    "            dept.append(a)\n",
    "        elif \"Department\" in a:\n",
    "            dept.append(a)\n",
    "        elif a in [\"World Economic Forum\",\"Bank Of England\",'Center For Global Development']:\n",
    "            uni.append(a)\n",
    "    return uni, dept, unis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "1bd9a059-35a7-45fa-93ee-a3a176b8a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in np.sort(list(set([authors[a][\"affiliation\"] for a in authors]))):\n",
    "#     uni, dept, unis = cleanAffiliation(\"\", i, unis)\n",
    "#     print(uni, dept,i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "68da1a56-29d1-4845-af5b-020429eb2ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515321"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for author in authors:\n",
    "    authors[author][\"observatory\"] = \"https://www.economicsobservatory.com/\" + re.sub(\n",
    "        \" \", \"-\", authors[author][\"name\"].lower()\n",
    "    )\n",
    "    (\n",
    "        authors[author][\"institution\"],\n",
    "        authors[author][\"department\"],\n",
    "        unis,\n",
    "    ) = cleanAffiliation(authors[author][\"name\"], authors[author][\"affiliation\"], unis)\n",
    "    authors[author]['articleCount']=len(authors[author]['articles'])\n",
    "open(\"authors.json\", \"w\").write(json.dumps(list(authors.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a126a81-06a3-4da3-b575-b17f0cee8556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
